# CITA-AIC Usage Guide

## üìö Table of Contents
1. [Quick Start](#quick-start)
2. [Preparing Your Dataset](#preparing-your-dataset)
3. [Training Your Model](#training-your-model)
4. [Making Predictions](#making-predictions)
5. [Understanding the Code](#understanding-the-code)

---

## üöÄ Quick Start

### Step 1: Prepare Your Images

**Option A: Use Webcam Capture Tool (Recommended for Facial Recognition)**

```bash
python webcam_capture.py
```

- Press number keys (1-5) to capture images for different emotions
- Aim for 50-100 images per category
- Images are automatically organized in `data/train/`

**Option B: Manual Organization**

Organize your images into folders by category:

```
data/train/
‚îú‚îÄ‚îÄ category1/
‚îÇ   ‚îú‚îÄ‚îÄ image1.jpg
‚îÇ   ‚îú‚îÄ‚îÄ image2.jpg
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ category2/
‚îÇ   ‚îú‚îÄ‚îÄ image1.jpg
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ category3/
    ‚îî‚îÄ‚îÄ ...
```

**Minimum Requirements:**
- At least 2 categories (can have more)
- Minimum 50 images per category (100-200 recommended)
- Images in JPG, PNG, or JPEG format

### Step 2: Train Your Model

```bash
python train.py
```

This will:
- ‚úÖ Validate your dataset structure
- ‚úÖ Load and preprocess images
- ‚úÖ Build the neural network
- ‚úÖ Train the model (10-30 minutes)
- ‚úÖ Save the trained model to `models/image_classifier.keras`
- ‚úÖ Generate training visualizations

### Step 3: Make Predictions

```bash
# Predict a single image
python predict.py path/to/image.jpg

# Predict all images in a folder
python predict.py path/to/folder/

# Interactive mode (predict multiple images)
python predict.py --interactive
```

---

## üìÅ Preparing Your Dataset

### Option 1: Webcam Capture Tool (For Facial Recognition)

Perfect for creating custom facial expression datasets!

**Step 1: Launch the tool**
```bash
python webcam_capture.py
```

**Step 2: Follow the on-screen instructions**
- Press **1** for Happy üòä
- Press **2** for Sad üò¢
- Press **3** for Neutral üòê
- Press **4** for Angry üò†
- Press **5** for Surprised üò≤
- Press **Q** to quit
- Press **S** to switch camera (if multiple cameras)

**Step 3: Capture guidelines**
- **Target:** 50-100 images per emotion
- **Vary your expressions:** Don't make the exact same face
- **Good lighting:** Face a window or turn on lights
- **Center your face:** Keep within the green rectangle
- **Different angles:** Slight head movements are good

**Step 4: Images auto-saved**
Images are automatically saved to:
```
data/train/happy/
data/train/sad/
data/train/neutral/
data/train/angry/
data/train/surprised/
```

### Option 2: Collect Your Own Images

1. **Choose your categories** (e.g., dog breeds, flower types, emotions)

2. **Gather images:**
   - Download from Google Images
   - Take your own photos
   - Use public datasets (Kaggle, ImageNet)
   - Web scraping (ensure compliance)

3. **How many images do you need?**
   - **Minimum:** 50 images per category
   - **Good:** 100-200 images per category
   - **Great:** 500+ images per category
   - **Rule:** More images = better accuracy!

4. **Image quality tips:**
   - ‚úÖ Clear, well-lit images
   - ‚úÖ Variety (angles, backgrounds, lighting)
   - ‚úÖ Similar sizes (or let preprocessing handle it)
   - ‚ùå Avoid blurry or heavily filtered images
   - ‚ùå Don't include mislabeled images

### Option 3: Use Pre-Made Datasets

Download datasets to practice:

**Kaggle Datasets (requires free account):**
- [Dogs vs Cats](https://www.kaggle.com/c/dogs-vs-cats/data)
- [Flowers Recognition](https://www.kaggle.com/alxmamaev/flowers-recognition)
- [Fruits 360](https://www.kaggle.com/moltean/fruits)
- [FER-2013 Facial Expressions](https://www.kaggle.com/datasets/msambare/fer2013)

**Steps:**
1. Download and extract dataset
2. Move images to `data/train/` with proper folder structure
3. Run `python train.py`

### Dataset Management Tools

**Split dataset into train/test:**
```python
from utils.preprocessor import split_dataset

split_dataset(
    source_dir='data/raw',
    train_dir='data/train',
    test_dir='data/test',
    test_split=0.2  # 20% for testing
)
```

**Clean dataset (remove corrupted images):**
```python
from utils.preprocessor import clean_dataset

stats = clean_dataset(
    'data/train',
    min_size=(50, 50),
    remove_invalid=True
)
```

**Analyze dataset:**
```python
from utils.preprocessor import analyze_dataset

analyze_dataset('data/train')
```

---

## üéØ Training Your Model

### Basic Training

Simply run:
```bash
python train.py
```

**The script will automatically:**
1. ‚úì Validate dataset structure
2. ‚úì Load and preprocess images
3. ‚úì Apply data augmentation (if enabled)
4. ‚úì Build model architecture
5. ‚úì Train for specified epochs
6. ‚úì Save best model
7. ‚úì Generate visualizations

### Training Configuration

Edit `config.py` before training:

```python
# KEY SETTINGS

# Training duration
EPOCHS = 15                    # Number of training cycles
BATCH_SIZE = 32               # Images processed together
LEARNING_RATE = 0.001         # How fast the model learns

# Model architecture
MODEL_TYPE = 'transfer_learning'  # or 'custom_cnn'
PRETRAINED_MODEL = 'MobileNetV2'  # MobileNetV2, ResNet50, EfficientNetB0

# Data augmentation
USE_DATA_AUGMENTATION = True      # Highly recommended

# Regularization
DROPOUT_RATE = 0.4            # Prevents overfitting (0.0-0.5)
```

### Training Modes Explained

#### **Mode 1: Transfer Learning (Recommended)**

**Best for:** Small-to-medium datasets (100-2000 images)

```python
MODEL_TYPE = 'transfer_learning'
PRETRAINED_MODEL = 'MobileNetV2'
FREEZE_BASE_MODEL = True
```

**Advantages:**
- ‚úÖ Fast training (10-15 minutes)
- ‚úÖ High accuracy with less data
- ‚úÖ Pre-trained on 1.4M images
- ‚úÖ Industry standard

**Available Models:**
- **MobileNetV2**: Lightweight, fast (recommended)
- **ResNet50**: More accurate, slower
- **EfficientNetB0**: Best balance

#### **Mode 2: Custom CNN (From Scratch)**

**Best for:** Large datasets (2000+ images), learning

```python
MODEL_TYPE = 'custom_cnn'
```

**Advantages:**
- ‚úÖ Full control over architecture
- ‚úÖ Great for learning
- ‚úÖ No external dependencies

**Disadvantages:**
- ‚è±Ô∏è Slower training (30-60 minutes)
- üìä Needs more data

### Monitoring Training

**Good Training Example:**
```
Epoch 10/15
accuracy: 0.92, val_accuracy: 0.89  ‚úÖ Small gap
loss: 0.25, val_loss: 0.30
```

**Overfitting Example (Bad):**
```
Epoch 10/15
accuracy: 0.98, val_accuracy: 0.65  ‚ùå Large gap
loss: 0.10, val_loss: 0.85
```

### Training Callbacks

Automatic features:
1. **Model Checkpoint**: Saves best model
2. **Early Stopping**: Stops if no improvement (5 epochs)
3. **Reduce Learning Rate**: Decreases LR when plateau
4. **TensorBoard Logging**: Records all metrics

### Expected Training Times

| Dataset Size | Model Type | Hardware | Time |
|--------------|-----------|----------|------|
| 250 images | Transfer Learning | CPU | 10-15 min |
| 500 images | Transfer Learning | CPU | 15-20 min |
| 500 images | Custom CNN | CPU | 30-45 min |
| 2000 images | Transfer Learning | GPU | 5-10 min |

### After Training

**Check results:**
1. **Training plots:** `results/plots/training_history.png`
2. **Confusion matrix:** `results/plots/confusion_matrix.png`
3. **Saved model:** `models/image_classifier.keras`
4. **Class names:** `models/class_names.txt`

---

## üîÆ Making Predictions

### Single Image Prediction

```bash
python predict.py path/to/image.jpg
```

**Example output:**
```
‚úì Model loaded successfully
‚úì Loaded 5 classes: ['angry', 'happy', 'neutral', 'sad', 'surprised']

Predicting: path/to/image.jpg
‚úì Prediction complete!
  Predicted class: happy
  Confidence: 95.43%
  
  All class probabilities:
    happy: 95.43%
    neutral: 2.31%
    sad: 1.42%
    surprised: 0.58%
    angry: 0.26%
```

### Batch Predictions

```bash
python predict.py path/to/folder/
```

**This will:**
- Process all images
- Show predictions for each
- Generate summary report
- Save to `results/predictions/batch_prediction_report.txt`

### Interactive Mode

```bash
python predict.py --interactive
```

**Prompt:**
```
Enter image path (or folder): image.jpg
‚úì Prediction complete!

Enter image path (or folder): [type another or 'quit']
```

### Programmatic Usage

```python
from predict import load_trained_model, load_class_names, predict_single_image

# Load once
model = load_trained_model()
class_names = load_class_names()

# Predict many times
result = predict_single_image(model, 'image.jpg', class_names, show_plot=False)

print(f"Predicted: {result['predicted_class']}")
print(f"Confidence: {result['confidence']:.2f}%")

# Get all probabilities
for class_name, prob in result['all_probabilities'].items():
    print(f"{class_name}: {prob*100:.2f}%")
```

### Confidence Thresholds

Adjust in `config.py`:
```python
CONFIDENCE_THRESHOLD = 0.6  # 60% minimum
```

**Predictions below threshold get warning:**
```
‚ö†Ô∏è Warning: Confidence below threshold (60.00%)
   The model is uncertain about this prediction.
```

### Understanding Confidence

**High Confidence (> 80%):**
- ‚úÖ Model is very certain
- ‚úÖ Usually accurate
- ‚úÖ Safe to trust

**Medium Confidence (60-80%):**
- ‚ö†Ô∏è Model is somewhat certain
- ‚ö†Ô∏è Verify with judgment
- ‚ö†Ô∏è May need review

**Low Confidence (< 60%):**
- ‚ùå Model is uncertain
- ‚ùå May be incorrect
- ‚ùå Image unlike training data

---

## üí° Understanding the Code

### Project Structure

```
CITA-AIC/
‚îÇ
‚îú‚îÄ‚îÄ config.py              # All settings
‚îú‚îÄ‚îÄ model.py               # Neural networks
‚îú‚îÄ‚îÄ train.py               # Training script
‚îú‚îÄ‚îÄ predict.py             # Prediction script
‚îú‚îÄ‚îÄ webcam_capture.py      # Data collection
‚îÇ
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py    # Image loading
‚îÇ   ‚îî‚îÄ‚îÄ preprocessor.py   # Dataset tools
‚îÇ
‚îú‚îÄ‚îÄ data/train/           # Your images
‚îú‚îÄ‚îÄ models/               # Saved models
‚îî‚îÄ‚îÄ results/              # Outputs
```

### Key Concepts

#### 1. **Convolutional Neural Network (CNN)**

**What it is:**
- Neural network designed for images
- Learns features automatically
- Processes through multiple layers

**How it works:**
```
Input Image (224x224x3)
    ‚Üì
Conv Layers (detect edges, shapes)
    ‚Üì
Pooling (reduce size, keep important features)
    ‚Üì
Dense Layers (make decision)
    ‚Üì
Output (class probabilities)
```

#### 2. **Transfer Learning**

**What it is:**
- Using pre-trained model as starting point
- Model knows basic features already
- We teach it our specific categories

**Benefits:**
- ‚úÖ 10x faster training
- ‚úÖ Better accuracy with less data
- ‚úÖ Pre-trained on 1.4M images
- ‚úÖ Industry standard

#### 3. **Data Augmentation**

**What it is:**
- Creating variations of images
- Helps model generalize

**Transformations:**
- Rotate ¬±20¬∞
- Flip horizontally
- Zoom in/out
- Shift position
- Adjust brightness

**Why it helps:**
- ‚úÖ Increases effective dataset size
- ‚úÖ Prevents overfitting
- ‚úÖ Makes model robust

#### 4. **Epochs**

**What it is:**
- One complete pass through data
- Model updates after each batch

**Guidelines:**
- Small dataset: 20-25 epochs
- Medium dataset: 15-20 epochs
- Large dataset: 10-15 epochs

#### 5. **Batch Size**

**What it is:**
- Images processed together
- Affects speed and memory

**Trade-offs:**
- Small (16): Less memory, slower
- Large (64): More memory, faster
- Recommended: 32 (good balance)

#### 6. **Learning Rate**

**What it is:**
- Step size for weight adjustments
- Controls learning speed

**Guidelines:**
- Default: 0.001
- Fine-tuning: 0.0001
- Fast training: 0.01


## ‚öôÔ∏è Customization Guide

### By Number of Classes

#### **2 Classes (Binary)**
```python
# No changes needed - auto-detected
EPOCHS = 15
MODEL_TYPE = 'transfer_learning'
```
**Examples:** Dogs vs Cats, Pass vs Fail

#### **3-5 Classes**
```python
EPOCHS = 15
MODEL_TYPE = 'transfer_learning'
USE_DATA_AUGMENTATION = True
```
**Examples:** Rock-Paper-Scissors, 5 Emotions

#### **6-10 Classes**
```python
EPOCHS = 20
DROPOUT_RATE = 0.5
USE_DATA_AUGMENTATION = True
```
**Examples:** 10 Dog Breeds, 7 Emotions

#### **10+ Classes**
```python
EPOCHS = 25
LEARNING_RATE = 0.0001
USE_DATA_AUGMENTATION = True
DROPOUT_RATE = 0.5
```
**Examples:** 100 Bird Species, 50 Car Models

### By Dataset Size

#### **Tiny (< 200 images)**
```python
MODEL_TYPE = 'transfer_learning'
FREEZE_BASE_MODEL = True
USE_DATA_AUGMENTATION = True
EPOCHS = 25
DROPOUT_RATE = 0.5
VALIDATION_SPLIT = 0.3
```
**Expected accuracy:** 60-75%

#### **Small (200-1000 images)**
```python
MODEL_TYPE = 'transfer_learning'
FREEZE_BASE_MODEL = True
USE_DATA_AUGMENTATION = True
EPOCHS = 20
DROPOUT_RATE = 0.4
```
**Expected accuracy:** 75-90%

#### **Medium (1000-5000 images)**
```python
MODEL_TYPE = 'transfer_learning'
FREEZE_BASE_MODEL = True
USE_DATA_AUGMENTATION = True
EPOCHS = 15
```
**Expected accuracy:** 85-95%

#### **Large (5000+ images)**
```python
MODEL_TYPE = 'transfer_learning'
PRETRAINED_MODEL = 'EfficientNetB0'
FREEZE_BASE_MODEL = False
USE_DATA_AUGMENTATION = False
EPOCHS = 10
BATCH_SIZE = 64
```
**Expected accuracy:** 90-98%

### By Hardware

#### **Low-End CPU (< 4GB RAM)**
```python
BATCH_SIZE = 8
IMG_HEIGHT = 128
IMG_WIDTH = 128
PRETRAINED_MODEL = 'MobileNetV2'
```

#### **Standard CPU (8GB RAM)**
```python
BATCH_SIZE = 32
IMG_HEIGHT = 224
IMG_WIDTH = 224
MODEL_TYPE = 'transfer_learning'
```

#### **GPU or High-End CPU**
```python
BATCH_SIZE = 64
IMG_HEIGHT = 224
IMG_WIDTH = 224
PRETRAINED_MODEL = 'EfficientNetB0'
```

### Pre-trained Model Comparison

| Model | Speed | Accuracy | Size | Best For |
|-------|-------|----------|------|----------|
| **MobileNetV2** | ‚ö°‚ö°‚ö° | ‚òÖ‚òÖ‚òÖ | 3.5M | Beginners, limited hardware |
| **EfficientNetB0** | ‚ö°‚ö° | ‚òÖ‚òÖ‚òÖ‚òÖ | 5.3M | Balanced performance |
| **ResNet50** | ‚ö° | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ | 25.6M | Maximum accuracy |

---

## üîß Troubleshooting

### 1. Out of Memory Error

**Error:**
```
ResourceExhaustedError: OOM when allocating tensor
```

**Solutions:**

**A. Reduce batch size:**
```python
BATCH_SIZE = 16  # Down from 32
```

**B. Reduce image size:**
```python
IMG_HEIGHT = 128
IMG_WIDTH = 128
```

**C. Use lighter model:**
```python
PRETRAINED_MODEL = 'MobileNetV2'
```

### 2. Model Not Learning

**Symptoms:**
```
Epoch 10/15
accuracy: 0.35, val_accuracy: 0.32  ‚Üê Stuck
```

**Solutions:**

**A. Check dataset:**
```bash
ls data/train/  # Verify folders exist
```

**B. Use transfer learning:**
```python
MODEL_TYPE = 'transfer_learning'
```

**C. Increase epochs:**
```python
EPOCHS = 25
```

**D. Verify images:**
```python
from utils.preprocessor import analyze_dataset
analyze_dataset('data/train')
```

### 3. Overfitting

**Symptoms:**
```
accuracy: 0.98, val_accuracy: 0.62  ‚Üê Big gap!
```

**Solutions:**

**A. Enable augmentation:**
```python
USE_DATA_AUGMENTATION = True
```

**B. Increase dropout:**
```python
DROPOUT_RATE = 0.5
```

**C. Reduce epochs:**
```python
EPOCHS = 10
```

**D. Collect more data:**
```bash
python webcam_capture.py
```

### 4. Training Too Slow

**Solutions:**

**A. Reduce image size:**
```python
IMG_HEIGHT = 128
IMG_WIDTH = 128
```

**B. Increase batch size:**
```python
BATCH_SIZE = 64
```

**C. Use faster model:**
```python
PRETRAINED_MODEL = 'MobileNetV2'
```

### 5. Low Prediction Confidence

**Symptoms:**
```
Confidence: 42.31%  ‚Üê Too low
```

**Solutions:**

**A. Train longer:**
```python
EPOCHS = 25
```

**B. Check image quality:**
```python
from utils.preprocessor import clean_dataset
clean_dataset('data/train', remove_invalid=True)
```

**C. Collect more data:**
- Aim for 100+ images per class

**D. Fine-tune model:**
```python
from model import unfreeze_base_model
import tensorflow as tf

model = tf.keras.models.load_model('models/image_classifier.keras')
model = unfreeze_base_model(model, layers_to_unfreeze=20)
```

### 6. Webcam Not Working

**Error:**
```
Cannot open camera
```

**Solutions:**

**A. Check permissions:**
- Windows: Settings ‚Üí Privacy ‚Üí Camera
- Mac: System Preferences ‚Üí Camera

**B. Close other apps:**
- Zoom, Skype, Teams

**C. Switch camera:**
- Press 'S' during capture

**D. Reinstall OpenCV:**
```bash
pip uninstall opencv-python
pip install opencv-python
```

### 7. Path Not Found

**Error:**
```
FileNotFoundError: data/train/happy/image.jpg
```

**Solutions:**

**A. Check directory:**
```bash
cd D:\Ai-Image-Classifier\Ai-Image-Classifier
pwd  # Verify location
```

**B. List files:**
```bash
dir data\train\happy
```

**C. Use correct filename:**
- Copy exact filename from dir output

---

## üöÄ Advanced Features

### Fine-Tuning

After initial training, unlock more layers:

```python
from model import unfreeze_base_model
from tensorflow import keras

# Load trained model
model = keras.models.load_model('models/image_classifier.keras')

# Unfreeze last 20 layers
model = unfreeze_base_model(model, layers_to_unfreeze=20)

# Train 5-10 more epochs
# (LR automatically reduced to 0.0001)
```

**When to use:**
- Validation accuracy > 70%
- Want to improve further
- Have time for extra training

### Custom Data Augmentation

**By use case:**

**Facial expressions:**
```python
AUGMENTATION_CONFIG = {
    'rotation_range': 10,
    'horizontal_flip': True,
    'vertical_flip': False,
    'brightness_range': [0.8, 1.2]
}
```

**Natural objects:**
```python
AUGMENTATION_CONFIG = {
    'rotation_range': 30,
    'horizontal_flip': True,
    'vertical_flip': True,
    'zoom_range': 0.3
}
```

**Documents/text:**
```python
AUGMENTATION_CONFIG = {
    'rotation_range': 5,
    'horizontal_flip': False,
    'brightness_range': [0.9, 1.1]
}
```

### Model Ensemble

Combine multiple models:

```python
import numpy as np
from tensorflow import keras

# Load models
models = [
    keras.models.load_model('models/mobilenet.keras'),
    keras.models.load_model('models/resnet.keras'),
    keras.models.load_model('models/efficientnet.keras')
]

# Predict with all
predictions = [model.predict(image) for model in models]

# Average predictions
final = np.mean(predictions, axis=0)
predicted_class = np.argmax(final)
```

**Benefits:**
- ‚úÖ More robust
- ‚úÖ Higher accuracy
- ‚úÖ Reduces errors

**Drawbacks:**
- ‚è±Ô∏è 3x slower inference
- üíæ More storage

### TensorBoard Visualization

```bash
tensorboard --logdir=logs/fit
```

Open `http://localhost:6006`

**Features:**
- Training curves
- Loss curves
- Learning rate changes
- Weight distributions

### Export for Mobile/Web

**TensorFlow Lite (mobile):**
```python
import tensorflow as tf

model = tf.keras.models.load_model('models/image_classifier.keras')
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

with open('models/model.tflite', 'wb') as f:
    f.write(tflite_model)
```

**TensorFlow.js (web):**
```bash
pip install tensorflowjs
tensorflowjs_converter --input_format=keras models/image_classifier.keras models/tfjs_model/
```

---

## üéì Real-World Example

### Facial Expression Recognition Demo

#### **Dataset Collection (15 min)**

```bash
python webcam_capture.py
```

**Data collected:**
- 50 images √ó 5 emotions = 250 total
- Emotions: Happy, Sad, Neutral, Angry, Surprised
- Good lighting, varied expressions

#### **Configuration**

```python
MODEL_TYPE = 'transfer_learning'
PRETRAINED_MODEL = 'MobileNetV2'
FREEZE_BASE_MODEL = True
EPOCHS = 15
BATCH_SIZE = 32
USE_DATA_AUGMENTATION = True
VALIDATION_SPLIT = 0.2
```

#### **Training (12 min)**

```bash
python train.py
```

**Results:**
- Training accuracy: 87%
- Validation accuracy: 44%
- Status: Overfitting (small dataset)

#### **Prediction Test**

```bash
python predict.py data/train/happy/happy_20251228_123319_476100.jpg
```

**Output:**
```
Predicted class: happy
Confidence: 47.89%

All probabilities:
  happy: 47.89%    ‚Üê Correct!
  sad: 25.86%
  surprised: 15.19%
  neutral: 9.54%
  angry: 1.53%
```

#### **Analysis**

**What worked:**
- ‚úÖ Webcam tool easy to use
- ‚úÖ Transfer learning with small dataset
- ‚úÖ Correctly identified emotions
- ‚úÖ Data augmentation helped

**What needs improvement:**
- üìä More data (100+ per emotion)
- üéØ Higher dropout (0.5)
- ‚è±Ô∏è More epochs (20-25)
- üí° Better lighting
- üé® More expression variety

#### **Expected Improvements**

| Images/Class | Expected Accuracy |
|--------------|-------------------|
| 50 (current) | 44% validation |
| 100 | 60-70% |
| 200 | 75-85% |
| 500+ | 85-95% |

---

## üìä Performance Benchmarks

### Training Time by Dataset Size

| Images | Model | Hardware | Time |
|--------|-------|----------|------|
| 250 | MobileNetV2 | CPU 4-core | 10-15 min |
| 250 | ResNet50 | CPU 4-core | 25-35 min |
| 500 | MobileNetV2 | CPU 8-core | 15-25 min |
| 1000 | MobileNetV2 | GPU RTX 3060 | 5-8 min |

### Memory Requirements

| Config | Min RAM | Recommended |
|--------|---------|-------------|
| Batch 16, 128√ó128 | 2 GB | 4 GB |
| Batch 32, 224√ó224 | 4 GB | 8 GB |
| Batch 64, 224√ó224 | 8 GB | 16 GB |

### Expected Accuracy

| Scenario | Accuracy |
|----------|----------|
| Well-separated classes, 100+ images | 90-98% |
| Similar classes, 100+ images | 75-85% |
| Small dataset (< 50 per class) | 60-75% |
| Very similar classes | 70-85% |

---

## üìù Best Practices

### Data Collection

‚úÖ **DO:**
- Collect 100-200 images per category
- Use consistent good lighting
- Vary expressions/angles slightly
- Balance classes (similar counts)
- Use high-quality images

‚ùå **DON'T:**
- Use blurry images
- Have huge imbalances (50 vs 500)
- Include mislabeled images
- Use exact duplicates
- Rush collection

### Training

‚úÖ **DO:**
- Start with transfer learning
- Enable augmentation for small datasets
- Monitor validation metrics
- Use early stopping
- Test on unseen data

‚ùå **DON'T:**
- Train on test data
- Ignore overfitting
- Use tiny learning rates (< 1e-7)
- Stop training too early
- Train repeatedly without changes

### Prediction

‚úÖ **DO:**
- Test on diverse images
- Set confidence thresholds
- Handle low confidence gracefully
- Verify predictions make sense
- Monitor distributions

‚ùå **DON'T:**
- Assume 100% accuracy
- Deploy without testing
- Ignore warnings
- Use overfitted models
- Trust blindly

---

## üéì Learning Resources

### Video Courses
- [3Blue1Brown Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
- [Fast.ai Practical Deep Learning](https://course.fast.ai/)

### Interactive
- [CNN Explainer](https://poloclub.github.io/cnn-explainer/)
- [TensorFlow Playground](https://playground.tensorflow.org/)

### Documentation
- [TensorFlow Tutorials](https://www.tensorflow.org/tutorials)
- [Keras Documentation](https://keras.io/guides/)

### Practice Projects

**Beginner:**
1. Dogs vs Cats
2. Rock-Paper-Scissors
3. Happy vs Sad faces

**Intermediate:**
4. 5-class flower recognition
5. 10 hand gestures
6. Food classification

**Advanced:**
7. 50+ bird species
8. Car model classification
9. Medical images

---

## ‚úÖ Success Checklist

- [ ] Dataset organized correctly
- [ ] At least 50 images per class
- [ ] Images clear and labeled
- [ ] Training completes without errors
- [ ] Validation accuracy > 70%
- [ ] Model saved successfully
- [ ] Predictions work on new images
- [ ] Confidence reasonable (> 60%)
- [ ] Training plots reviewed
- [ ] Confusion matrix checked

---

## üéâ Congratulations!

You've mastered CITA-AIC! You now know:

‚úÖ Data collection (webcam + manual)
‚úÖ Dataset preparation and organization
‚úÖ Training configuration
‚úÖ Neural network architectures
‚úÖ Making predictions
‚úÖ Troubleshooting issues
‚úÖ Performance optimization
‚úÖ Advanced features

**Next steps:**
1. Build your own classifier
2. Experiment with datasets
3. Share your results
4. Contribute improvements

